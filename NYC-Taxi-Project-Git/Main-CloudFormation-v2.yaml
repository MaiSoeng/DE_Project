AWSTemplateFormatVersion: "2010-09-09"
Description: CloudFormation template for Project NYC Taxi

Parameters:
  ProjectName:
    Type: String
    Default: project-nyc-taxi
    Description: Project name

  VpcId:
    Type: AWS::EC2::VPC::Id
    Description: Select a VPC with Subnets covering 2 Availability zone at least.
    MinLength: 1

  SubnetIds:
    Type: List<AWS::EC2::Subnet::Id>
    Description: Select two private subnets covering 2 Availability zone at least.
    MinLength: 2

  SubnetAzs:
    Type: List<String>
    Description: AZ names corresponding to SubnetIds

  DBName:
    Type: String
    Default: taxidata
    Description: The database name to store NYC taxi data

  DBUsername:
    NoEcho: true
    Type: String
    Default: usermi
    Description: The username for my database

  DBPassword:
    NoEcho: true
    Type: String
    Description: The password of the database
    MinLength: 8
    MaxLength: 41

  EnableKinesisStreaming:
    Type: String
    Default: 'false'
    AllowedValues: [ 'true', 'false' ]
    Description: Optionally enable Kinesis Streaming for real-time data ingestion

  ExistingS3EndpointId:
    Type: String
    Default: 'vpce-0040fd9b8ebf9ecf8'
    Description: ID of existing S3 Gateway VPC Endpoint

  ExistingSecretsManagerEndpointId:
    Type: String
    Default: 'vpce-04c2f6209f4f97946'
    Description: ID of existing Secrets Manager Interface VPC Endpoint

  ExistingMonitoringEndpointId:
    Type: String
    Default: 'vpce-00daba060f81791cd'
    Description: ID of existing CloudWatch Monitoring Interface VPC Endpoint

Resources:
  # Create an S3 bucket for raw NYC Taxi data
  RawDataBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub '${ProjectName}-raw-data-${AWS::AccountId}'
      VersioningConfiguration:
        Status: Enabled
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true
      NotificationConfiguration:
        EventBridgeConfiguration:
          EventBridgeEnabled: true
      LifecycleConfiguration:
        Rules:
        - Id: DeleteOldVersions
          Status: Enabled
          NoncurrentVersionExpiration:
            NoncurrentDays: 90
        - Id: GlacierRule
          Prefix: glacier/
          Status: Enabled
          ExpirationInDays: 365
          Transitions:
          - TransitionInDays: 90
            StorageClass: GLACIER_IR

  # Create an S3 bucket for processed NYC Taxi data
  ProcessedDataBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub '${ProjectName}-processed-data-${AWS::AccountId}'
      VersioningConfiguration:
        Status: Enabled
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true
      LifecycleConfiguration:
        Rules:
        - Id: DeleteOldVersions
          Status: Enabled
          NoncurrentVersionExpiration:
            NoncurrentDays: 90

  # Create an S3 bucket for Glue ETL and temp files
  GlueBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub '${ProjectName}-glue-${AWS::AccountId}'
      VersioningConfiguration:
        Status: Enabled
      PublicAccessBlockConfiguration:
        BlockPublicAcls: true
        BlockPublicPolicy: true
        IgnorePublicAcls: true
        RestrictPublicBuckets: true

  # DB Subnet Group
  DBSubnetGroup:
    Type: AWS::RDS::DBSubnetGroup
    Properties:
      DBSubnetGroupDescription: Subnet Group for MySQL database
      SubnetIds: !Ref SubnetIds
      Tags:
      - Key: Name
        Value: !Sub '${ProjectName}-database-subnet-group'

  # Secrect Manager for DB Password
  DBPasswordSecret:
    Type: AWS::SecretsManager::Secret
    Properties:
      Name: !Sub '${ProjectName}-database-password'
      Description: Database Password for the NYC Taxi Project
      SecretString: !Sub |
        {
          "username": "${DBUsername}",
          "password": "${DBPassword}"
        }

  # Security Group for RDS
  MyRDSSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Security Group for RDS MySQL
      VpcId: !Ref VpcId
      SecurityGroupIngress:
      - IpProtocol: tcp
        FromPort: 3306
        ToPort: 3306
        SourceSecurityGroupId: !Ref GlueSecurityGroup
        Description: Allow AWS Glue ETL to access RDS MySQL
      Tags:
      - Key: Name
        Value: !Sub '${ProjectName}-database-security-group'

  # Security Group for AWS Glue
  GlueSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Security Group for Glue ETL
      VpcId: !Ref VpcId
      SecurityGroupEgress:
      - IpProtocol: -1
        CidrIp: 0.0.0.0/0
        Description: Allow all outbound traffic
      Tags:
      - Key: Name
        Value: !Sub '${ProjectName}-glue-security-group'

  # Self-referencing rule for Glue SG
  # This is to make sure that driver and executor can communicate
  GlueSecurityGroupIngress:
    Type: AWS::EC2::SecurityGroupIngress
    Properties:
      GroupId: !Ref GlueSecurityGroup
      IpProtocol: -1
      SourceSecurityGroupId: !Ref GlueSecurityGroup
      Description: Allow communication within Glue security group

  # Allow Glue job to access 443
  VPCEndpointSG:
    Type: AWS::EC2::SecurityGroup
    Properties:
      VpcId: !Ref VpcId
      GroupDescription: Allow Glue job to reach interface endpoints
      SecurityGroupIngress:
      - IpProtocol: tcp
        FromPort: 443
        ToPort: 443
        SourceSecurityGroupId: !Ref GlueSecurityGroup
      SecurityGroupEgress:
      - IpProtocol: -1
        CidrIp: 0.0.0.0/0
      Tags:
      - Key: Name
        Value: !Sub '${ProjectName}-vpce-sg'

  # VPC Endpoints Configuration
  # Using existing endpoints where possible to avoid duplication and costs

  # Create Glue endpoint
  GlueEndpoint:
    Type: AWS::EC2::VPCEndpoint
    Properties:
      VpcId: !Ref VpcId
      ServiceName: !Sub 'com.amazonaws.${AWS::Region}.glue'
      VpcEndpointType: Interface
      PrivateDnsEnabled: true
      SubnetIds: !Ref SubnetIds
      SecurityGroupIds: [ !Ref VPCEndpointSG ]
      Tags:
      - Key: Name
        Value: !Sub '${ProjectName}-vpce-glue'

  # Create CloudWatch Logs endpoint
  CloudWatchLogsEndpoint:
    Type: AWS::EC2::VPCEndpoint
    Properties:
      VpcId: !Ref VpcId
      ServiceName: !Sub 'com.amazonaws.${AWS::Region}.logs'
      VpcEndpointType: Interface
      PrivateDnsEnabled: true
      SubnetIds: !Ref SubnetIds
      SecurityGroupIds: [ !Ref VPCEndpointSG ]
      Tags:
      - Key: Name
        Value: !Sub '${ProjectName}-vpce-logs'

  # Create S3 Interface endpoint
  S3Endpoint:
    Type: AWS::EC2::VPCEndpoint
    Properties:
      VpcId: !Ref VpcId
      ServiceName: !Sub 'com.amazonaws.${AWS::Region}.s3'
      VpcEndpointType: Interface
      PrivateDnsEnabled: true
      SubnetIds: !Ref SubnetIds
      SecurityGroupIds: [ !Ref VPCEndpointSG ]
      PolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal: '*'
          Action:
          - s3:GetObject
          - s3:PutObject
          - s3:DeleteObject
          - s3:ListBucket
          Resource:
          - !Sub 'arn:aws:s3:::${RawDataBucket}/*'
          - !Sub 'arn:aws:s3:::${ProcessedDataBucket}/*'
          - !Sub 'arn:aws:s3:::${GlueBucket}/*'
          - !GetAtt RawDataBucket.Arn
          - !GetAtt ProcessedDataBucket.Arn
          - !GetAtt GlueBucket.Arn
      Tags:
      - Key: Name
        Value: !Sub '${ProjectName}-vpce-s3'

  # Create Secrets Manager endpoint
  SecretsManagerEndpoint:
    Type: AWS::EC2::VPCEndpoint
    Properties:
      VpcId: !Ref VpcId
      ServiceName: !Sub 'com.amazonaws.${AWS::Region}.secretsmanager'
      VpcEndpointType: Interface
      PrivateDnsEnabled: true
      SubnetIds: !Ref SubnetIds
      SecurityGroupIds: [ !Ref VPCEndpointSG ]
      Tags:
      - Key: Name
        Value: !Sub '${ProjectName}-vpce-secretsmanager'

  # Create CloudWatch Monitoring endpoint
  CloudWatchMonitoringEndpoint:
    Type: AWS::EC2::VPCEndpoint
    Properties:
      VpcId: !Ref VpcId
      ServiceName: !Sub 'com.amazonaws.${AWS::Region}.monitoring'
      VpcEndpointType: Interface
      PrivateDnsEnabled: true
      SubnetIds: !Ref SubnetIds
      SecurityGroupIds: [ !Ref VPCEndpointSG ]
      Tags:
      - Key: Name
        Value: !Sub '${ProjectName}-vpce-monitoring'

  # Lambda function to download NYC Taxi data from official TLC source
  DataDownloadLambda:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub '${ProjectName}-data-downloader'
      Runtime: python3.9
      Handler: index.lambda_handler
      Role: !GetAtt DataDownloadRole.Arn
      Timeout: 900 #
      MemorySize: 1024
      Environment:
        Variables:
          RAW_DATA_BUCKET: !Ref RawDataBucket
          PROJECT_NAME: !Ref ProjectName
      Code:
        ZipFile: |
          import json
          import boto3
          import os
          import logging
          import urllib.request
          import urllib.error
          from datetime import datetime, timedelta

          logger = logging.getLogger()
          logger.setLevel(logging.INFO)

          s3_client = boto3.client('s3')

          def discover_available_data():
              """Yellow Taxi data for 2025 only"""
              available_files = []
              
              # Generate list for all 2025 months
              year = 2025
              logger.info(f"Prepare to download {year} Yellow Taxi data")
              
              # Try each month 
              for month in range(1, 13):
                  month_str = f"{month:02d}"
                  filename = f"yellow_tripdata_{year}-{month_str}.parquet"
                  
                  available_files.append({
                      'filename': filename,
                      'year': year,
                      'month': month
                  })
              
              logger.info(f"Will attempt to download files for {year}")
              return available_files

          def check_existing_files(bucket_name, available_files):
              """Check which files already exist in S3 to avoid re-downloading"""
              existing_files = set()
              
              try:
                  # List existing files in data/ prefix
                  response = s3_client.list_objects_v2(
                      Bucket = bucket_name,
                      Prefix = 'data/',
                      MaxKeys = 1000
                  )
                  
                  if 'Contents' in response:
                      for obj in response['Contents']:
                          filename = obj['Key'].split('/')[-1]
                          existing_files.add(filename)
                          
              except Exception as e:
                  logger.warning(f"Could not check existing files: {str(e)}")
              
              # Filter out files that already exist
              new_files = []
              for file_info in available_files:
                  if file_info['filename'] not in existing_files:
                      new_files.append(file_info)
                  else:
                      logger.info(f"File already exists, skipping: {file_info['filename']}")
              
              return new_files

          def download_file_from_url(url, filename, bucket_name, s3_key):
              """Download file from URL and upload to S3"""
              try:
                  logger.info(f"Downloading from {url}")
                  
                  # Download file from URL with headers
                  request = urllib.request.Request(url)
                  request.add_header('User-Agent', 'Mozilla/5.0')
                  
                  with urllib.request.urlopen(request, timeout = 300) as response:
                      file_data = response.read()
                      logger.info(f"Downloaded {len(file_data)} bytes")
                      
                      # Upload to S3
                      s3_client.put_object(
                          Bucket = bucket_name,
                          Key = s3_key,
                          Body = file_data,
                          ServerSideEncryption = 'AES256'
                      )
                      logger.info(f"Successfully uploaded {filename} to s3://{bucket_name}/{s3_key}")
                      return True
                      
              except urllib.error.HTTPError as e:
                  logger.error(f"HTTP Error {e.code} for {filename}: {e.reason}")
                  return False
              except urllib.error.URLError as e:
                  logger.error(f"URL Error for {filename}: {str(e)}")
                  return False
              except Exception as e:
                  logger.error(f"Failed to download {filename}: {str(e)}")
                  return False

          def lambda_handler(event, context):
              try:
                  logger.info("Start 2025 NYC Yellow Taxi data discovery and download")
                  logger.info("Use HTTPS download from NYC TLC official website")
                  
                  bucket_name = os.environ['RAW_DATA_BUCKET']
                  
                  # Discover available 2025 data
                  logger.info("Discover available 2025 Yellow Taxi data")
                  available_files = discover_available_data()
                  
                  if not available_files:
                      logger.warning("No 2025 Yellow Taxi data found")
                      return {
                          'statusCode': 200,
                          'body': json.dumps({
                              'message': 'No 2025 Yellow Taxi data found',
                              'files_count': 0
                          })
                      }
                  
                  logger.info(f"Found {len(available_files)} available 2025 files")
                  
                  # Check which files already exist
                  logger.info("Check existing files")
                  new_files = check_existing_files(bucket_name, available_files)
                  
                  if not new_files:
                      logger.info("All available files already exist in S3")
                      return {
                          'statusCode': 200,
                          'body': json.dumps({
                              'message': 'All available files already exist',
                              'files_count': 0,
                              'available_files': len(available_files)
                          })
                      }
                  
                  logger.info(f"Found {len(new_files)} new files to download")
                  
                  # Download new files
                  logger.info("Download new files")
                  downloaded_files = []
                  
                  for file_info in new_files:
                          filename = file_info['filename']
                          year = file_info['year']
                          month = file_info['month']
                          
                          logger.info(f"Processing: {filename} (Year: {year}, Month: {month})")
                          
                          # NYC TLC official data URL
                          base_url = "https://d37ci6vzurychx.cloudfront.net/trip-data"
                          download_url = f"{base_url}/{filename}"
                          s3_key = f"data/{filename}"
                          
                          # Download and upload to S3
                          if download_file_from_url(download_url, filename, bucket_name, s3_key):
                              downloaded_files.append(s3_key)
                          else:
                              logger.info(f"Skip {filename} - file may not be available yet")
                  
                  logger.info(f"Download completed. S3 events will trigger crawler")
                  
                  return {
                      'statusCode': 200,
                      'body': json.dumps({
                          'message': '2025 NYC Yellow Taxi data download completed',
                          'downloaded_files': downloaded_files,
                          'files_count': len(downloaded_files),
                          'available_files': len(available_files),
                          'skipped_files': len(available_files) - len(new_files),
                          'data_source': 'NYC TLC 2025 Yellow Taxi'
                      })
                  }
                  
              except Exception as e:
                  logger.error(f"Error in data download: {str(e)}")
                  return {
                      'statusCode': 500,
                      'body': json.dumps({
                          'error': str(e)
                      })
                  }

  # IAM Role for Data Download Lambda
  DataDownloadRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub '${ProjectName}-data-download-role'
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service: lambda.amazonaws.com
          Action: sts:AssumeRole
      ManagedPolicyArns:
      - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
      - PolicyName: S3Access
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action:
            - s3:PutObject
            - s3:PutObjectAcl
            - s3:GetObject
            - s3:ListBucket
            - s3:CopyObject
            - s3:GetBucketLocation
            Resource:
            - !Sub 'arn:aws:s3:::${RawDataBucket}/*'
            - !GetAtt RawDataBucket.Arn

  # EventBridge Rule for scheduled data download
  DataDownloadSchedule:
    Type: AWS::Events::Rule
    Properties:
      Name: !Sub '${ProjectName}-data-download-schedule'
      Description: Schedule NYC Taxi data download from official TLC source
      ScheduleExpression: 'cron(0 9 1 * ? *)' # Run monthly on 1st at 9 AM UTC
      State: ENABLED
      Targets:
      - Arn: !GetAtt DataDownloadLambda.Arn
        Id: DataDownloadTarget

  # Permission for EventBridge to invoke Lambda
  DataDownloadSchedulePermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref DataDownloadLambda
      Action: lambda:InvokeFunction
      Principal: events.amazonaws.com
      SourceArn: !GetAtt DataDownloadSchedule.Arn

  # Custom Resource Lambda to trigger data download on stack creation/update
  TriggerDataDownloadLambda:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub '${ProjectName}-trigger-data-download'
      Runtime: python3.9
      Handler: index.lambda_handler
      Role: !GetAtt TriggerDataDownloadRole.Arn
      Timeout: 60
      MemorySize: 256
      Code:
        ZipFile: |
          import json
          import boto3
          import logging
          import urllib3

          logger = logging.getLogger()
          logger.setLevel(logging.INFO)

          lambda_client = boto3.client('lambda')
          http = urllib3.PoolManager()

          def send_response(event, context, response_status, response_data):
              """Send response to CloudFormation"""
              response_body = json.dumps({
                  'Status': response_status,
                  'Reason': f'See CloudWatch Log Stream: {context.log_stream_name}',
                  'PhysicalResourceId': context.log_stream_name,
                  'StackId': event['StackId'],
                  'RequestId': event['RequestId'],
                  'LogicalResourceId': event['LogicalResourceId'],
                  'Data': response_data
              })
              
              try:
                  http.request('PUT', event['ResponseURL'], body=response_body,
                              headers={'Content-Type': 'application/json'})
                  logger.info('Response sent to CloudFormation')
              except Exception as e:
                  logger.error(f'Failed to send response: {str(e)}')

          def lambda_handler(event, context):
              logger.info(f'Received event: {json.dumps(event)}')
              
              try:
                  request_type = event['RequestType']
                  data_download_function = event['ResourceProperties']['DataDownloadFunction']
                  
                  if request_type == 'Delete':
                      logger.info('Delete request - no action needed')
                      send_response(event, context, 'SUCCESS', {'Message': 'Resource deleted'})
                      return
                  
                  if request_type in ['Create', 'Update']:
                      logger.info(f'Triggering data download Lambda: {data_download_function}')
                      
                      # Invoke the data download Lambda asynchronously
                      response = lambda_client.invoke(
                          FunctionName=data_download_function,
                          InvocationType='Event'  # Asynchronous invocation
                      )
                      
                      logger.info(f'Data download Lambda triggered successfully')
                      send_response(event, context, 'SUCCESS', {
                          'Message': 'Data download Lambda triggered',
                          'StatusCode': response['StatusCode']
                      })
                      return
                      
              except Exception as e:
                  logger.error(f'Error: {str(e)}')
                  send_response(event, context, 'FAILED', {'Message': str(e)})

  # IAM Role for Trigger Lambda
  TriggerDataDownloadRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub '${ProjectName}-trigger-download-role'
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service: lambda.amazonaws.com
          Action: sts:AssumeRole
      ManagedPolicyArns:
      - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
      - PolicyName: InvokeLambda
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action:
            - lambda:InvokeFunction
            Resource:
            - !GetAtt DataDownloadLambda.Arn

  # Custom Resource to trigger data download on stack creation
  TriggerDataDownload:
    Type: Custom::TriggerDataDownload
    DependsOn:
    - DataDownloadLambda
    - RawDataBucket
    Properties:
      ServiceToken: !GetAtt TriggerDataDownloadLambda.Arn
      DataDownloadFunction: !Ref DataDownloadLambda

  # RDS MySQL Database
  AuroraClusterParameterGroup:
    Type: AWS::RDS::DBClusterParameterGroup
    Properties:
      Description: !Sub "${ProjectName} Aurora cluster parameters"
      Family: aurora-mysql8.0
      Parameters:
        time_zone: "UTC"

  # Aurora Serverless Cluster
  AuroraCluster:
    Type: AWS::RDS::DBCluster
    Properties:
      Engine: aurora-mysql
      DatabaseName: !Ref DBName
      DBClusterIdentifier: !Sub "${ProjectName}-aurora-cluster"
      DBSubnetGroupName: !Ref DBSubnetGroup
      VpcSecurityGroupIds:
      - !GetAtt MyRDSSecurityGroup.GroupId
      DBClusterParameterGroupName: !Ref AuroraClusterParameterGroup
      DeletionProtection: false
      EnableHttpEndpoint: true
      StorageEncrypted: true
      MasterUsername: !Ref DBUsername
      MasterUserPassword: !Ref DBPassword
      ServerlessV2ScalingConfiguration:
        MinCapacity: 8
        MaxCapacity: 64

  AuroraWriterInstance:
    Type: AWS::RDS::DBInstance
    Properties:
      DBClusterIdentifier: !Ref AuroraCluster
      DBInstanceIdentifier: !Sub "${ProjectName}-aurora-writer"
      DBInstanceClass: db.serverless
      Engine: aurora-mysql
      EnablePerformanceInsights: true
      PubliclyAccessible: false
      AutoMinorVersionUpgrade: true

  AuroraReaderInstance:
    Type: AWS::RDS::DBInstance
    Properties:
      DBClusterIdentifier: !Ref AuroraCluster
      DBInstanceIdentifier: !Sub "${ProjectName}-aurora-reader"
      DBInstanceClass: db.serverless
      Engine: aurora-mysql
      PubliclyAccessible: false

  # create policy for Glue job
  GlueJobRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub '${ProjectName}-glue-service-role'
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service: glue.amazonaws.com
          Action: sts:AssumeRole
      ManagedPolicyArns:
      - arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole
      Policies:
      - PolicyName: S3Access
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action:
            - s3:GetObject
            - s3:PutObject
            - s3:DeleteObject
            - s3:ListBucket
            - s3:GetBucketLocation
            Resource:
            - !Sub 'arn:aws:s3:::${RawDataBucket}/*'
            - !Sub 'arn:aws:s3:::${ProcessedDataBucket}/*'
            - !Sub 'arn:aws:s3:::${GlueBucket}/*'
            - !GetAtt RawDataBucket.Arn
            - !GetAtt ProcessedDataBucket.Arn
            - !GetAtt GlueBucket.Arn

      - PolicyName: VPCAccess
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action:
            - ec2:CreateNetworkInterface
            - ec2:DeleteNetworkInterface
            - ec2:DescribeNetworkInterfaces
            - ec2:DescribeVpcs
            - ec2:DescribeSubnets
            - ec2:DescribeSecurityGroups
            - ec2:AttachNetworkInterface
            - ec2:DetachNetworkInterface
            - ec2:CreateTags
            Resource: '*'

      - PolicyName: DQDLAccess
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action:
            - glue:GetDataQualityResult
            - glue:StartDataQualityEvaluation
            - glue:GetDataQualityRuleset
            - glue:CreateDataQualityRuleset
            - glue:UpdateDataQualityRuleset
            - glue:ListDataQualityResults
            - glue:GetDataQualityRuleRecommendationRun
            - glue:StartDataQualityRuleRecommendationRun
            - glue:DeleteDataQualityRuleset
            Resource: '*'

      - PolicyName: PassRole
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action: iam:PassRole
            Resource:
            - !Sub 'arn:aws:iam::${AWS::AccountId}:role/${ProjectName}-glue-service-role'
            - !Sub 'arn:aws:iam::${AWS::AccountId}:role/${ProjectName}--*'
            Condition:
              StringEquals:
                iam:PassedToService: glue.amazonaws.com

      - PolicyName: CloudWatch
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action:
            - logs:CreateLogGroup
            - logs:CreateLogStream
            - logs:PutLogEvents
            - logs:DescribeLogGroups
            - logs:DescribeLogStreams
            - cloudwatch:PutMetricData
            Resource: '*'

      - PolicyName: SecretsManagerAccess
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action:
            - secretsmanager:GetSecretValue
            - secretsmanager:DescribeSecret
            Resource:
            - !Sub arn:aws:secretsmanager:${AWS::Region}:${AWS::AccountId}:secret:${ProjectName}-database-password*

  # Glue Database
  GlueDatabase:
    Type: AWS::Glue::Database
    Properties:
      CatalogId: !Ref AWS::AccountId
      DatabaseInput:
        Name: !Sub '${ProjectName}_taxi_database'
        Description: NYC taxi database for the project
        LocationUri: !Sub 's3://${RawDataBucket}/'

  # Create a Glue crawler
  GlueCrawler:
    Type: AWS::Glue::Crawler
    Properties:
      Name: !Sub '${ProjectName}_raw-data-crawler'
      Description: Crawler for hw16 NYC taxi data
      Role: !GetAtt GlueJobRole.Arn
      DatabaseName: !Ref GlueDatabase
      SchemaChangePolicy:
        UpdateBehavior: UPDATE_IN_DATABASE
        DeleteBehavior: DEPRECATE_IN_DATABASE
      Targets:
        S3Targets:
        - Path: !Sub 's3://${RawDataBucket}/data/'
          Exclusions:
          - 'Zones/*'

  # Create a glue ETL job script editor
  GlueJob:
    Type: AWS::Glue::Job
    Properties:
      Name: !Sub '${ProjectName}-etl-job'
      Role: !GetAtt GlueJobRole.Arn
      GlueVersion: "5.0"
      WorkerType: G.1X
      NumberOfWorkers: 8
      Timeout: 2880
      MaxRetries: 1
      Connections:
        Connections:
        - !Ref GlueConnection
      Command:
        Name: glueetl
        ScriptLocation: !Sub 's3://${GlueBucket}/scripts/taxi_etl.py'
      DefaultArguments:
        '--job-language': python
        '--TempDir': !Sub 's3://${GlueBucket}/temp/'
        '--raw-data-bucket': !Ref RawDataBucket
        '--processed-data-bucket': !Ref ProcessedDataBucket
        '--enable-metrics': 'true'
        '--enable-continuous-cloudwatch-log': 'true'
        '--enable-spark-ui': 'true'
        '--spark-event-logs-path': !Sub 's3://${GlueBucket}/spark-logs/'
        '--job-bookmark-option': 'job-bookmark-enable'
        '--enable-glue-datacatalog': 'true'
        '--database-name': !Sub '${ProjectName}_taxi_database'
        '--rds-endpoint': !GetAtt AuroraCluster.Endpoint.Address
        '--rds-port': !GetAtt AuroraCluster.Endpoint.Port
        '--rds_username': !Ref DBUsername
        '--rds_password': !Sub '${ProjectName}-database-password'
        '--rds-database': !Ref DBName
      ExecutionProperty:
        MaxConcurrentRuns: 1

  # Glue Connection for VPC
  GlueConnection:
    Type: AWS::Glue::Connection
    Properties:
      CatalogId: !Ref AWS::AccountId
      ConnectionInput:
        Name: !Sub '${ProjectName}-vpc-connection'
        ConnectionType: NETWORK
        PhysicalConnectionRequirements:
          AvailabilityZone: !Select [ 0, !Ref SubnetAzs ]
          SecurityGroupIdList:
          - !Ref GlueSecurityGroup
          SubnetId: !Select [ 0, !Ref SubnetIds ]

  # Create glue job logs
  GlueJobLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub '/aws-glue/jobs/${ProjectName}-etl-job'
      RetentionInDays: 30

  # CloudWatch Alarm for Glue Job Failures
  GlueJobFailureAlarm:
    Type: AWS::CloudWatch::Alarm
    Properties:
      AlarmName: !Sub '${ProjectName}-glue-job-failure'
      AlarmDescription: This is the alarm for ETL job failures
      MetricName: glue.driver.aggregate.numFailedTasks
      Namespace: Glue
      Statistic: Sum
      Period: 300
      EvaluationPeriods: 1
      Threshold: 1
      ComparisonOperator: GreaterThanOrEqualToThreshold
      Dimensions:
      - Name: JobName
        Value: !Ref GlueJob
      - Name: JobRunId
        Value: ALL

  # IAM Role for Crawler Automation Lambda Function
  CrawlerLambdaRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub '${ProjectName}-crawler-lambda-role'
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service: lambda.amazonaws.com
          Action: sts:AssumeRole
      ManagedPolicyArns:
      - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
      - PolicyName: GlueCrawlerAccess
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action:
            - glue:StartCrawler
            - glue:GetCrawler
            - glue:GetCrawlerMetrics
            - glue:StopCrawler
            - glue:StartJobRun
            - glue:GetJobRun
            - glue:GetJob
            Resource: '*'
          - Effect: Allow
            Action:
            - logs:CreateLogGroup
            - logs:CreateLogStream
            - logs:PutLogEvents
            Resource: '*'

  # Lambda function to start the crawler automatically
  CrawlerLambdaFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub '${ProjectName}-crawler-automation'
      Runtime: python3.9
      Role: !GetAtt CrawlerLambdaRole.Arn
      Handler: index.lambda_handler
      Timeout: 300
      MemorySize: 256
      Environment:
        Variables:
          CRAWLER_NAME: !Ref GlueCrawler
          ETL_NAME: !Ref GlueJob
          PROJECT_NAME: !Ref ProjectName
      Code:
        ZipFile: |
          import json
          import boto3
          import os
          import logging
          from datetime import datetime

          logger = logging.getLogger()
          logger.setLevel(logging.INFO)

          glue_client = boto3.client('glue')

          def lambda_handler(event, context):
            try:
                crawler_name = os.environ['CRAWLER_NAME']
                etl_name = os.environ['ETL_NAME']

                logger.info(f"Starting crawler automation for: {crawler_name}")

                # check the crawler status
                crawler_response = glue_client.get_crawler(Name = crawler_name)
                crawler_state = crawler_response['Crawler']['State']

                if crawler_state == 'RUNNING':
                  logger.info(f"{crawler_name} is already running")
                  return {
                      'statusCode': 200,
                      'body': json.dumps(f'{crawler_name} is already running')
                  }
                
                # start the crawler
                logger.info(f"Starting crawler: {crawler_name}")
                glue_client.start_crawler(Name = crawler_name)

                # check if we should also start Glue ETL
                if 'detail' in event and 'object' in event['detail']:
                  bucket = event['detail']['bucket']['name']
                  key = event['detail']['object']['key']
                  logger.info(f"New object detected: s3://{bucket}/{key}")

                elif 'Records' in event and event['Records']:
                  for record in event['Records']:
                    if 'eventName' in record and record['eventName'].startswith('ObjectCreated'):
                      bucket = record['s3']['bucket']['name']
                      key = record['s3']['object']['key']
                      logger.info(f"New object detected: s3://{bucket}/{key}")

                      logger.info("Scheduling ETL job after the crawler completes")
                
                return {
                    'statusCode': 200,
                    'body': json.dumps(f'Successfully started crawler {crawler_name}') 
                }
            
            except Exception as e:
              logger.error(f"Error in crawler automation: {str(e)}")
              return {
                'statusCode': 500,
                'body': json.dumps({"error": str(e)})
              }

  # EventBridge for S3
  S3ObjectCreationRule:
    Type: AWS::Events::Rule
    Properties:
      Name: !Sub '${ProjectName}-s3-object-created'
      Description: Trigger the crawler when new data is stored in S3
      EventPattern:
        source:
        - aws.s3
        detail-type:
        - Object Created
        detail:
          bucket:
            name:
            - !Ref RawDataBucket
          object:
            key:
            - suffix: '.parquet'
      State: ENABLED
      Targets:
      - Arn: !GetAtt CrawlerLambdaFunction.Arn
        Id: CrawlerLambdaTarget

  # Permission for EventBridge to invoke Lambda
  S3EventLambdaPermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref CrawlerLambdaFunction
      Action: lambda:InvokeFunction
      Principal: events.amazonaws.com
      SourceArn: !GetAtt S3ObjectCreationRule.Arn

  # EventBridge Rule for scheduled crawler execution
  ScheduledCrawlerRule:
    Type: AWS::Events::Rule
    Properties:
      Name: !Sub '${ProjectName}-scheduled-crawler'
      Description: Run the crawler on a specific schedule
      ScheduleExpression: 'cron(0 2 * * ? *)' # crawl the data at 2am UTC
      State: ENABLED
      Targets:
      - Arn: !GetAtt CrawlerLambdaFunction.Arn
        Id: ScheduledCrawlerTarget

  # Permission for the rule to invoke lambda
  ScheduledCrawlerPermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref CrawlerLambdaFunction
      Action: lambda:InvokeFunction
      Principal: events.amazonaws.com
      SourceArn: !GetAtt ScheduledCrawlerRule.Arn

  # EventBridge Rule for ETL job after crawler completion
  CrawlerCompletionRule:
    Type: AWS::Events::Rule
    Properties:
      Name: !Sub '${ProjectName}-crawler-completion'
      Description: Trigger ETL job when crawler completes
      EventPattern:
        source:
        - aws.glue
        detail-type:
        - Glue Crawler State Change
        detail:
          crawlerName:
          - !Ref GlueCrawler
          state:
          - Succeeded
      State: ENABLED
      Targets:
      - Arn: !GetAtt ETLJobTriggerLambda.Arn
        Id: ETLJobTriggerTarget

  # Lambda function to trigger ETL job after crawler completes
  ETLJobTriggerLambda:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: !Sub '${ProjectName}-etl-job-trigger'
      Runtime: python3.9
      Role: !GetAtt CrawlerLambdaRole.Arn
      Handler: index.lambda_handler
      Timeout: 300
      MemorySize: 256
      Environment:
        Variables:
          ETL_NAME: !Ref GlueJob
          PROJECT_NAME: !Ref ProjectName
          DB_SECRET_NAME: !Sub '${ProjectName}-database-password'
      Code:
        ZipFile: |+
          import json
          import boto3
          import os
          import logging
          from datetime import datetime

          logger = logging.getLogger()
          logger.setLevel(logging.INFO)

          glue_client = boto3.client('glue')

          def lambda_handler(event, context):
            try:
              etl_name = os.environ['ETL_NAME']
              logger.info(f"Crawler completes successfully, start: {etl_name}")

              # check the status of the ETL job
              progress = {"STARTING", "RUNNING", "STOPPING"}
              try:
                response = glue_client.get_job_runs(JobName = etl_name, MaxResults = 10)
                job_runs = response.get('JobRuns', [])
                if any(j.get('JobRunState') in progress for j in job_runs):
                  logger.info(f"ETL job {etl_name} in progress")
                  return {
                    'statusCode': 200,
                    'body': json.dumps(f'ETL job {etl_name} in progress')
                  }
              
              except Exception as e:
                logger.warning(f"Could not check the ETL status: {e}")

              run = glue_client.start_job_run(JobName = etl_name, Arguments={'--rds_password': os.environ['DB_SECRET_NAME']})
              logger.info(f"ETL {etl_name} starts")
              return {
                'statusCode': 200,
                'body': json.dumps(f'ETL {etl_name} starts')
              }
            
            except Exception as e:
              logger.exception("Error starting ETL job")
              return {'statusCode': 500, 'body': json.dumps(str(e))}

  # Permission for EventBridge to invoke ETL job trigger lambda function
  ETLLambdaPermission:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref ETLJobTriggerLambda
      Action: lambda:InvokeFunction
      Principal: events.amazonaws.com
      SourceArn: !GetAtt CrawlerCompletionRule.Arn

  # QuickSight IAM Role for RDS access
  QuickSightRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub '${ProjectName}-quicksight-rds-role'
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service: quicksight.amazonaws.com
          Action: sts:AssumeRole
      Policies:
      - PolicyName: RDSReadAccess
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action:
            - rds:DescribeDBInstances
            - rds:DescribeDBClusters
            - rds:DescribeDBEngineVersions
            - rds:DescribeDBParameterGroups
            - rds:DescribeDBSecurityGroups
            - rds:DescribeDBSubnetGroups
            Resource: '*'
      - PolicyName: SecretsManagerAccess
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action:
            - secretsmanager:GetSecretValue
            - secretsmanager:DescribeSecret
            Resource:
            - !Sub arn:aws:secretsmanager:${AWS::Region}:${AWS::AccountId}:secret:${ProjectName}-database-password*

  # Note: QuickSight Data Source needs to be created manually due to permission requirements
  # The following resources provide the necessary infrastructure for manual QuickSight setup

  # QuickSight VPC Connection for accessing RDS in private subnet
  QuickSightVPCConnection:
    Type: AWS::QuickSight::VPCConnection
    Properties:
      AwsAccountId: !Ref AWS::AccountId
      VPCConnectionId: !Sub '${ProjectName}-vpc-connection'
      Name: !Sub '${ProjectName}-VPC-Connection'
      SubnetIds: !Ref SubnetIds
      SecurityGroupIds:
      - !Ref QuickSightSecurityGroup
      RoleArn: !GetAtt QuickSightVPCRole.Arn
      DnsResolvers:
      - 169.254.169.253
      Tags:
      - Key: Name
        Value: !Sub '${ProjectName}-quicksight-vpc-connection'

  # IAM Role for QuickSight VPC Connection
  QuickSightVPCRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Sub '${ProjectName}-quicksight-vpc-role'
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service: quicksight.amazonaws.com
          Action: sts:AssumeRole
      Policies:
      - PolicyName: EC2NetworkAccess
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action:
            - ec2:DescribeVpcs
            - ec2:DescribeSubnets
            - ec2:DescribeSecurityGroups
            - ec2:DescribeNetworkInterfaces
            - ec2:CreateNetworkInterface
            - ec2:DeleteNetworkInterface
            - ec2:ModifyNetworkInterfaceAttribute
            - ec2:DescribeNetworkInterfaceAttribute
            - ec2:AttachNetworkInterface
            - ec2:DetachNetworkInterface
            Resource: '*'
      - PolicyName: S3Access
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action:
            - s3:GetObject
            - s3:ListBucket
            Resource: '*'

  # Security Group for QuickSight VPC Connection
  QuickSightSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Security Group for QuickSight VPC Connection to access RDS
      VpcId: !Ref VpcId
      SecurityGroupIngress:
      - IpProtocol: tcp
        FromPort: 3306
        ToPort: 3306
        SourceSecurityGroupId: !Ref MyRDSSecurityGroup
        Description: Allow QuickSight to access RDS MySQL
      SecurityGroupEgress:
      - IpProtocol: -1
        CidrIp: 0.0.0.0/0
        Description: Allow all outbound traffic
      Tags:
      - Key: Name
        Value: !Sub '${ProjectName}-quicksight-security-group'

  # Update RDS Security Group to allow QuickSight access
  RDSQuickSightIngress:
    Type: AWS::EC2::SecurityGroupIngress
    Properties:
      GroupId: !Ref MyRDSSecurityGroup
      IpProtocol: tcp
      FromPort: 3306
      ToPort: 3306
      SourceSecurityGroupId: !Ref QuickSightSecurityGroup
      Description: Allow QuickSight VPC Connection to access RDS

  # Kinesis - Optional
  KinesisStreaming:
    Type: AWS::CloudFormation::Stack
    Condition: EnableKinesis
    Properties:
      TemplateURL: !Sub 'https://${GlueBucket}.s3.amazonaws.com/templates/kinesis-stack.yaml'
      Parameters:
        ProjectName: !Ref ProjectName
        RawDataBucket: !Ref RawDataBucket

Conditions:
  EnableKinesis: !Equals [ !Ref EnableKinesisStreaming, 'true' ]

Outputs:
  RawDataBucket:
    Description: S3 bucket for raw taxi data
    Value: !Ref RawDataBucket
    Export:
      Name: !Sub '${ProjectName}-raw-data-bucket'

  ProcessedDataBucket:
    Description: S3 bucket for processed data
    Value: !Ref ProcessedDataBucket
    Export:
      Name: !Sub '${ProjectName}-processed-data-bucket'

  GlueBucket:
    Description: S3 bucket for Glue jobs
    Value: !Ref GlueBucket
    Export:
      Name: !Sub '${ProjectName}-glue-bucket'

  DatabasePort:
    Description: MySQL database port
    Value: !GetAtt AuroraCluster.Endpoint.Port
    Export:
      Name: !Sub '${ProjectName}-database-port'

  AuroraClusterEndpoint:
    Value: !GetAtt AuroraCluster.Endpoint.Address
    Description: Writer endpoint for Aurora cluster

  AuroraReaderEndpoint:
    Value: !GetAtt AuroraCluster.ReadEndpoint.Address
    Description: Reader endpoint for Aurora cluster

  DatabaseSecretArn:
    Description: ARN of the databasse password secret
    Value: !Ref DBPasswordSecret
    Export:
      Name: !Sub '${ProjectName}-database-secret-arn'

  GlueJobName:
    Description: Name of the Glue ETL job
    Value: !Ref GlueJob
    Export:
      Name: !Sub '${ProjectName}-etl-job-name'

  GlueDatabaseName:
    Description: Name of the Glue database
    Value: !Ref GlueDatabase
    Export:
      Name: !Sub '${ProjectName}-glue-database'

  CrawlerName:
    Description: Name of the crawler
    Value: !Ref GlueCrawler
    Export:
      Name: !Sub '${ProjectName}-crawler-name'

  CrawlerLambda:
    Description: ARN of the crawler lambda function
    Value: !GetAtt CrawlerLambdaFunction.Arn
    Export:
      Name: !Sub '${ProjectName}-crawler-lambda-arn'

  ETLJobTriggerLambda:
    Description: ARN of the ETL job trigger lambda function
    Value: !GetAtt ETLJobTriggerLambda.Arn
    Export:
      Name: !Sub '${ProjectName}-etl-trigger-lambda-arn'

  QuickSightVPCConnectionId:
    Description: ID of the QuickSight VPC connection
    Value: !Ref QuickSightVPCConnection
    Export:
      Name: !Sub '${ProjectName}-quicksight-vpc-connection-id'

  QuickSightRoleArn:
    Description: ARN of the QuickSight IAM role for RDS access
    Value: !GetAtt QuickSightRole.Arn
    Export:
      Name: !Sub '${ProjectName}-quicksight-role-arn'

  QuickSightVPCRoleArn:
    Description: ARN of the QuickSight VPC connection IAM role
    Value: !GetAtt QuickSightVPCRole.Arn
    Export:
      Name: !Sub '${ProjectName}-quicksight-vpc-role-arn'

  # Existing VPC Endpoints Information
  ExistingS3EndpointId:
    Description: ID of existing S3 Gateway VPC Endpoint
    Value: !Ref ExistingS3EndpointId

  ExistingSecretsManagerEndpointId:
    Description: ID of existing Secrets Manager Interface VPC Endpoint
    Value: !Ref ExistingSecretsManagerEndpointId

  ExistingMonitoringEndpointId:
    Description: ID of existing CloudWatch Monitoring Interface VPC Endpoint
    Value: !Ref ExistingMonitoringEndpointId

  # CloudFormation Created Endpoints
  GlueEndpointId:
    Description: ID of CloudFormation created Glue Interface VPC Endpoint
    Value: !Ref GlueEndpoint

  CloudWatchLogsEndpointId:
    Description: ID of CloudFormation created CloudWatch Logs Interface VPC Endpoint
    Value: !Ref CloudWatchLogsEndpoint

  # Data Download Lambda
  DataDownloadLambdaArn:
    Description: ARN of the NYC Taxi data download Lambda function
    Value: !GetAtt DataDownloadLambda.Arn
    Export:
      Name: !Sub '${ProjectName}-${AWS::StackName}-data-download-lambda-arn'

  DataDownloadScheduleArn:
    Description: ARN of the data download schedule
    Value: !GetAtt DataDownloadSchedule.Arn

  TriggerDataDownloadLambdaArn:
    Description: ARN of the trigger data download Lambda function (Custom Resource)
    Value: !GetAtt TriggerDataDownloadLambda.Arn
    Export:
      Name: !Sub '${ProjectName}-trigger-data-download-lambda-arn'
